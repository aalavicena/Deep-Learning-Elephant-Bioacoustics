{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01353a6a-5934-4024-875b-b032c592e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This notebook shows how to infer unseen data on the trained model for model evaluation purposes\n",
    "\n",
    "# required modules\n",
    "import math, random\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from IPython.display import Audio\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "\n",
    "data_path = ('PATH/TO/YOUR/DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19c1ba0-42bb-4b80-9249-69c01274dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before loading the state dictionary, which is simply an information of the parameters of the trained model,\n",
    "#-we need to call the functions again\n",
    "\n",
    "# 1. Pre-processing and DataLoader \n",
    "class AudioUtil():\n",
    "  @staticmethod\n",
    "  def open(audio_file):\n",
    "    sig, sr = torchaudio.load(audio_file)\n",
    "    return (sig, sr)\n",
    "  \n",
    "  # ----------------------------\n",
    "  # Convert the given audio to the desired number of channels\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def rechannel(aud, new_channel):\n",
    "    sig, sr = aud\n",
    "\n",
    "    if (sig.shape[0] == new_channel):\n",
    "      # Nothing to do\n",
    "      return aud\n",
    "\n",
    "    if (new_channel == 1):\n",
    "      # Convert from stereo to mono by selecting only the first channel\n",
    "      resig = sig[:1, :]\n",
    "    else:\n",
    "      # Convert from mono to stereo by duplicating the first channel\n",
    "      resig = torch.cat([sig, sig])\n",
    "\n",
    "    return ((resig, sr))\n",
    "\n",
    "  # ----------------------------\n",
    "  # Since Resample applies to a single channel, we resample one channel at a time\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def resample(aud, newsr):\n",
    "    sig, sr = aud\n",
    "\n",
    "    if (sr == newsr):\n",
    "      # Nothing to do\n",
    "      return aud\n",
    "\n",
    "    num_channels = sig.shape[0]\n",
    "    # Resample first channel\n",
    "    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "    if (num_channels > 1):\n",
    "      # Resample the second channel and merge both channels\n",
    "      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "      resig = torch.cat([resig, retwo])\n",
    "\n",
    "    return ((resig, newsr))\n",
    "    \n",
    "  # ----------------------------\n",
    "  # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def pad_trunc(aud, max_ms):\n",
    "    sig, sr = aud\n",
    "    num_rows, sig_len = sig.shape\n",
    "    max_len = sr//1000 * max_ms\n",
    "\n",
    "    if (sig_len > max_len):\n",
    "      # Truncate the signal to the given length\n",
    "      sig = sig[:,:max_len]\n",
    "\n",
    "    elif (sig_len < max_len):\n",
    "      # Length of padding to add at the beginning and end of the signal\n",
    "      pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "      pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "      # Pad with 0s\n",
    "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "      sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "      \n",
    "    return (sig, sr)\n",
    "  \n",
    "  # ----------------------------\n",
    "  # Shifts the signal to the left or right by some percent. Values at the end\n",
    "  # are 'wrapped around' to the start of the transformed signal.\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def time_shift(aud, shift_limit):\n",
    "    sig,sr = aud\n",
    "    _, sig_len = sig.shape\n",
    "    shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "    return (sig.roll(shift_amt), sr)\n",
    "\n",
    "  # ----------------------------\n",
    "  # Generate a Spectrogram\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "    sig,sr = aud\n",
    "    top_db = 80\n",
    "\n",
    "    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "\n",
    "    # Convert to decibels\n",
    "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "    return (spec)\n",
    "\n",
    "  # ----------------------------\n",
    "  # Augment the Spectrogram by masking out some sections of it in both the frequency\n",
    "  # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n",
    "  # overfitting and to help the model generalise better. The masked sections are\n",
    "  # replaced with the mean value.\n",
    "  # ----------------------------\n",
    "  @staticmethod\n",
    "  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "    _, n_mels, n_steps = spec.shape\n",
    "    mask_value = spec.mean()\n",
    "    aug_spec = spec\n",
    "\n",
    "    freq_mask_param = max_mask_pct * n_mels\n",
    "    for _ in range(n_freq_masks):\n",
    "      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    time_mask_param = max_mask_pct * n_steps\n",
    "    for _ in range(n_time_masks):\n",
    "      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    return aug_spec\n",
    "\n",
    "class SoundDS(Dataset):\n",
    "  def __init__(self, df, data_path):\n",
    "    self.df = df\n",
    "    self.data_path = str(data_path)\n",
    "    self.duration = 4000\n",
    "    self.sr = 44100\n",
    "    self.channel = 2\n",
    "    self.shift_pct = 0.4\n",
    "            \n",
    "  # ----------------------------\n",
    "  # Number of items in dataset\n",
    "  # ----------------------------\n",
    "  def __len__(self):\n",
    "    return len(self.df)    \n",
    "    \n",
    "  # ----------------------------\n",
    "  # Get i'th item in dataset\n",
    "  # ----------------------------\n",
    "  def __getitem__(self, idx):\n",
    "    # Absolute file path of the audio file - concatenate the audio directory with\n",
    "    # the relative path\n",
    "    audio_file = self.data_path + self.df.loc[idx, 'relative_path']\n",
    "    # Get the Class ID\n",
    "    ele = self.df.loc[idx, 'ele']\n",
    "\n",
    "    aud = AudioUtil.open(audio_file)\n",
    "    # Some sounds have a higher sample rate, or fewer channels compared to the\n",
    "    # majority. So make all sounds have the same number of channels and same \n",
    "    # sample rate. Unless the sample rate is the same, the pad_trunc will still\n",
    "    # result in arrays of different lengths, even though the sound duration is\n",
    "    # the same.\n",
    "    reaud = AudioUtil.resample(aud, self.sr)\n",
    "    rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "\n",
    "    dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "\n",
    "    sgram = AudioUtil.spectro_gram(dur_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "\n",
    "\n",
    "    return sgram, ele\n",
    "\n",
    "# 2. Metrics\n",
    "class Metric:\n",
    "    '''Metric computes accuracy/precision/recall/confusion_matrix with batch updates.'''\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.y = []\n",
    "        self.t = []\n",
    "\n",
    "    def update(self, y, t):\n",
    "        '''Update with batch outputs and labels.\n",
    "        Args:\n",
    "          y: (tensor) model outputs sized [N,].\n",
    "          t: (tensor) labels targets sized [N,].\n",
    "        '''\n",
    "        self.y.append(y)\n",
    "        self.t.append(t)\n",
    "\n",
    "    def _process(self, y, t):\n",
    "        '''Compute TP, FP, FN, TN.\n",
    "        Args:\n",
    "          y: (tensor) model outputs sized [N,].\n",
    "          t: (tensor) labels targets sized [N,].\n",
    "        Returns:\n",
    "          (tensor): TP, FP, FN, TN, sized [num_classes,].\n",
    "        '''\n",
    "        tp = torch.empty(self.num_classes)\n",
    "        fp = torch.empty(self.num_classes)\n",
    "        fn = torch.empty(self.num_classes)\n",
    "        tn = torch.empty(self.num_classes)\n",
    "        for i in range(self.num_classes):\n",
    "            tp[i] = ((y == i) & (t == i)).sum().item()\n",
    "            fp[i] = ((y == i) & (t != i)).sum().item()\n",
    "            fn[i] = ((y != i) & (t == i)).sum().item()\n",
    "            tn[i] = ((y != i) & (t != i)).sum().item()\n",
    "        return tp, fp, fn, tn\n",
    "\n",
    "    def accuracy(self, reduction='mean'):\n",
    "        '''Accuracy = (TP+TN) / (P+N).\n",
    "        Args:\n",
    "          reduction: (str) mean or none.\n",
    "        Returns:\n",
    "          (tensor) accuracy.\n",
    "        '''\n",
    "        if not self.y or not self.t:\n",
    "            return\n",
    "        assert(reduction in ['none', 'mean'])\n",
    "        y = torch.cat(self.y, 0)\n",
    "        t = torch.cat(self.t, 0)\n",
    "        tp, fp, fn, tn = self._process(y, t)\n",
    "        if reduction == 'none':\n",
    "            acc = tp / (tp + fn)\n",
    "        else:\n",
    "            acc = tp.sum() / (tp + fn).sum()\n",
    "        return acc\n",
    "\n",
    "    def precision(self, reduction='mean'):\n",
    "        '''Precision = TP / (TP+FP).\n",
    "        Args:\n",
    "          reduction: (str) mean or none.\n",
    "        Returns:\n",
    "          (tensor) precision.\n",
    "        '''\n",
    "        if not self.y or not self.t:\n",
    "            return\n",
    "        assert(reduction in ['none', 'mean'])\n",
    "        y = torch.cat(self.y, 0)\n",
    "        t = torch.cat(self.t, 0)\n",
    "        tp, fp, fn, tn = self._process(y, t)\n",
    "        prec = tp / (tp + fp)\n",
    "        prec[torch.isnan(prec)] = 0\n",
    "        if reduction == 'mean':\n",
    "            prec = prec.mean()\n",
    "        return prec\n",
    "\n",
    "    def recall(self, reduction='mean'):\n",
    "        '''Recall = TP / P.\n",
    "        Args:\n",
    "          reduction: (str) mean or none.\n",
    "        Returns:\n",
    "          (tensor) recall.\n",
    "        '''\n",
    "        if not self.y or not self.t:\n",
    "            return\n",
    "        assert(reduction in ['none', 'mean'])\n",
    "        y = torch.cat(self.y, 0)\n",
    "        t = torch.cat(self.t, 0)\n",
    "        tp, fp, fn, tn = self._process(y, t)\n",
    "        recall = tp / (tp + fn)\n",
    "        recall[torch.isnan(recall)] = 0\n",
    "        if reduction == 'mean':\n",
    "            recall = recall.mean()\n",
    "        return recall\n",
    "\n",
    "    def confusion_matrix(self):\n",
    "        y = torch.cat(self.y, 0)\n",
    "        t = torch.cat(self.t, 0)\n",
    "        matrix = torch.zeros(self.num_classes, self.num_classes)\n",
    "        for i in range(self.num_classes):\n",
    "            for j in range(self.num_classes):\n",
    "                matrix[j][i] = ((y == i) & (t == j)).sum().item()\n",
    "        return matrix\n",
    "\n",
    "\n",
    "def test():\n",
    "    import torchmetrics.functional as M\n",
    "    nc = 2\n",
    "    m = Metric(num_classes=nc)\n",
    "    y = torch.randint(0, nc, (2,))\n",
    "    t = torch.randint(0, nc, (2,))\n",
    "    m.update(y, t)\n",
    "\n",
    "    print('\\naccuracy:')\n",
    "    print(m.accuracy('none'))\n",
    "    print(M.accuracy(y, t, 'none', num_classes=2))\n",
    "    print(m.accuracy('mean'))\n",
    "    \n",
    "\n",
    "    print('\\nprecision:')\n",
    "    print(m.precision('none'))\n",
    "    print(M.precision(y, t, 'none', num_classes=2))\n",
    "    print(m.precision('mean'))\n",
    "   \n",
    "\n",
    "    print('\\nrecall:')\n",
    "    print(m.recall('none'))\n",
    "    print(M.recall(y, t, 'none', num_classes=2))\n",
    "    print(m.recall('mean'))\n",
    "    \n",
    "\n",
    "    print('\\nconfusion matrix:')\n",
    "    print(m.confusion_matrix())\n",
    "    print(M.confusion_matrix(y, t, num_classes=2))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b7511-cb5c-453d-b805-1ac536dd7178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initialize model architecture\n",
    "class AudioClassifier (nn.Module):\n",
    "    # ----------------------------\n",
    "    # Build the model architecture\n",
    "    # ----------------------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Third Convolution Block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        # Fourth Convolution Block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=64, out_features=10)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    " \n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Final output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bef1b8-dda0-40ef-a471-dd1db418d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model's state dictionary\n",
    "model = AudioClassifier()\n",
    "model.load_state_dict(torch.load('YOUR/MODEL.pt')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d919b874-47e6-45c3-a4f7-463bc6fc17af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process sound clips\n",
    "os.chdir('PATH/TO/YOUR/DATA')\n",
    "df = pd.read_csv('YOUR/CSV.csv')\n",
    "df.head()\n",
    "\n",
    "# Construct file path by concatenating datasetid and itemid\n",
    "df['relative_path'] = '/' + df['datasetid'].astype(str) + '/' + df['itemid'].astype(str)\n",
    "\n",
    "# Take relevant columns\n",
    "df = df[['relative_path', 'ele']]\n",
    "\n",
    "memeds = SoundDS(df, data_path)\n",
    "\n",
    "# Create test data loaders\n",
    "inf = torch.utils.data.DataLoader(memeds, batch_size=1, shuffle=False)\n",
    "print(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43788b6-da42-48ce-9dc4-4d2f9db0d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae858e2e-cfc9-493c-80e0-91ba0428ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for inference\n",
    "def inference (model, inf):\n",
    "  correct_prediction = 0\n",
    "  total_prediction = 0\n",
    "  metric = Metric(num_classes=2)\n",
    "\n",
    "  # Disable gradient updates\n",
    "  with torch.no_grad():\n",
    "    for images, labels in inf:\n",
    "\n",
    "      # Get the predicted class with the highest score\n",
    "        images,labels = images.to(device),labels.to(device)\n",
    "        output = model(images)\n",
    "        _, predictions = torch.max(output.data,1)\n",
    "        metric.update(predictions, labels)\n",
    "        correct_prediction += (predictions == labels).sum().item()\n",
    "        total_prediction += predictions.shape[0]\n",
    "        \n",
    "     \n",
    "    # Metrics\n",
    "    acc = metric.accuracy()*100\n",
    "    prec = metric.precision()\n",
    "    recall = metric.recall()\n",
    "    f1 = 2*prec*recall / (prec+recall)\n",
    "    confusion_matrix = metric.confusion_matrix()\n",
    "    print(f'Accuracy: {acc:.2f} | Precision: {prec:.2f} | Recall: {recall:.2f} | F1 Score: {f1:.2f} | Total items: {total_prediction}')\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np \n",
    "\n",
    "    ax = sns.heatmap(confusion_matrix, annot=True, cmap='Blues')\n",
    "\n",
    "    ax.set_title('CNN1 Model Confusion Matrix\\n\\n');\n",
    "    ax.set_xlabel('\\nPredicted Values')\n",
    "    ax.set_ylabel('Actual Values ');\n",
    "\n",
    "    # Ticket labels - List must be in alphabetical order\n",
    "    ax.xaxis.set_ticklabels(['False','True'])\n",
    "    ax.yaxis.set_ticklabels(['False','True'])\n",
    "    \n",
    "    # Display the visualization of the Confusion Matrix.\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "## Run inference on trained model with the validation set ##\n",
    "inference(model, inf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
